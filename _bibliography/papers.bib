---
---

@string{aps = {American Physical Society,}}

@inproceedings{you2024shiftaddllm,
  abbr={NeurIPS 2024}
  title={ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization},
  author={You, Haoran and Guo, Yipin and Fu, Yichao and Zhou, Wei and Shi, Huihong and Zhang, Xiaofan and Kundu, Souvik and Yazdanbakhsh, Amir and Lin, Yingyan},
  journal="NeurIPS",
  year="2024",
  code={https://github.com/GATECH-EIC/ShiftAddLLM?tab=readme-ov-file},
  pdf={https://arxiv.org/pdf/2406.05981}
}

@inproceedings{azizi2024lamda,
  title={LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation},
  author={Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud},
  journal={EMNLP (Findings)},
  year={2024}
}

@inproceedings{ramachandran2024clamp,
  title={CLAMP-ViT: contrastive data-free learning for adaptive post-training quantization of ViTs},
  author={Ramachandran, Akshat and Kundu, Souvik and Krishna, Tushar},
  journal={ECCV},
  year={2024}
}

@inproceedings{liu2024aflora,
  title={AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models},
  author={Kundu*, Souvik and Liu*, Zeyu and Li, Anni and Wan, Junrui and Jiang, Lianghao and Beerel, Peter Anthony},
  journal={ACL (best paper recommendation)},
  year={2024}
}

@article{yin2023junk,
  title={Junk dna hypothesis: A task-centric angle of llm pre-trained weights through sparsity},
  author={Yin, Lu and Liu, Shiwei and Jaiswal, Ajay and Kundu, Souvik and Wang, Zhangyang},
  journal={ICML},
  year={2024}
}

