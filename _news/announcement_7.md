---
layout: post
date: 2025-05-18 15:59:00-0400
inline: true
related_posts: false
---
<p>
 1x (<a target="_blank" href="https://icml.cc/virtual/2025/poster/43982"><b>On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention for Long-Context LLM Serving</b></a>) accepted at <a target="_blank" href="https://icml.cc/"><b>ICML 2025</b></a>, 1x (<a target="_blank" href="https://openreview.net/pdf?id=L0AEJlQKAB"><b>LAMB: A Training-Free Method to Enhance the Long-Context Understanding of SSMs via Attention-Guided Token Filtering</b></a>) accepted at <a target="_blank" href="https://2025.aclweb.org/"><b>ACL 2025</b></a>, 1x (<a target="_blank" href="https://arxiv.org/pdf/2504.14365"><b>Accelerating LLM Inference with Flexible N:M Sparsity via A
Fully Digital Compute-in-Memory Accelerator</b></a>) accepted at <a target="_blank" href="https://www.islped.org/2025/"><b>ISLPED 2025</b></a>! :sparkles:
</p>