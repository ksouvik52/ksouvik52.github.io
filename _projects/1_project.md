---
layout: page
title: Deep learning efficiency research
description: Post-training optimization, calibration, and reprameterization
img: assets/img/model_optimization_rep.jpg
importance: 1
category: work
related_publications: true
---

<p>
This research thrust focuses on democratizing AI via various optimization, calibration, and reprameterization of pre-training large foundation models for their efficient deployment on extremely resource limited hardware. Some of the research outcome of this project includes: <a target="_blank" href="https://arxiv.org/pdf/2406.05981"><b>ShiftAddLLM</b></a>, <a target="_blank" href="https://arxiv.org/pdf/2403.05527"><b>GEAR</b></a>, <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1145/3510833"><b>dynamic network rewiring</b></a>
</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/shiftaddllm.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/clampvit.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/sparselearning.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    We explore different avenues in yielding efficient models, including model reparameterization, quantization, sparse learning.
</div>

Some of the notable public highlights are: <a target="_blank" href="https://x.com/arankomatsuzaki/status/1767026050477760920"><b>GEAR highlight</b></a>, <a target="_blank" href="https://huggingface.co/papers?date=2024-06-11"><b>ShiftAddLLM highlight by AK in Huggingface</b></a>

